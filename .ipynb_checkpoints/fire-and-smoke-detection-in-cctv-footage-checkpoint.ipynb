{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Detection of fire and smoke in CCTV footage by fire authorities can significantly increase the response time to such tragedies saving many lives. This kernel translates this task into an image regonition problem on sampled CCTV video frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../input/fire-detection-from-cctv/data/data/video_data/test_videos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3dc29fdc5ddc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./archive/data/img_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/fire-detection-from-cctv/data/data/video_data/test_videos\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../input/fire-detection-from-cctv/data/data/video_data/test_videos'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)'\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./archive/data/img_data\"))\n",
    "print(os.listdir(\"../input/fire-detection-from-cctv/data/data/video_data/test_videos\"))\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "print(os.listdir(\"../input/resnet50\"))\n",
    "print(os.listdir(\"../input/vgg16\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The CCTV footage for training and testing is downloaded from publically available videos on Youtube. The training set is prepared by sampling the videos at a constant rate and manually distributing the sampled frames into 'default', 'fire' and 'smoke' categories. Frames are kept in directories named after the class to which  they belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "NUM_EPOCHS = 20\n",
    "NUM_CLASSES = 3\n",
    "TRAIN_BATCH_SIZE = 77\n",
    "TEST_BATCH_SIZE = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "The model is created by appending 'Dense' layer, with number of activation units equivalent to the number classes (3), to a pre-trained Reset50 model. This significantly reduces the training time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def create_model( model_size ):\n",
    "    my_new_model = Sequential()\n",
    "    if  model_size == 'L':\n",
    "        resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        resnet = ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path)\n",
    "        #resnet.summary()\n",
    "        my_new_model.add(resnet)\n",
    "        my_new_model.layers[0].trainable = False\n",
    "    else:\n",
    "        vgg_weights_path = '../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "        vgg= VGG16(include_top=False, weights=vgg_weights_path ) \n",
    "        vgg.summary()\n",
    "        my_new_model.add(vgg)\n",
    "        my_new_model.add(GlobalAveragePooling2D())\n",
    "        my_new_model.layers[0].trainable = False\n",
    "        my_new_model.layers[1].trainable = False\n",
    "        \n",
    "    my_new_model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "   \n",
    "    # Say no to train first layer (ResNet) model. It is already trained\n",
    "    \n",
    "    opt = optimizers.adam()\n",
    "    my_new_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return my_new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The frames for training and testing are read from the directories using ImageDataGenerator.Data augmentation is performed by horizontally flipping each image by setting the horizontal_flip to True in the ImageDataGenerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model( model ):\n",
    "    #ata_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "    data_generator_with_aug = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1,\n",
    "                                #sear_range=0.01,\n",
    "                                zoom_range=[0.9, 1.25],\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip=False,\n",
    "                                data_format='channels_last',\n",
    "                                brightness_range=[0.5, 1.5]\n",
    "                               )\n",
    "                                       \n",
    "    train_generator = data_generator_with_aug.flow_from_directory(\n",
    "            '../input/fire-detection-from-cctv/data/data/img_data/train',\n",
    "            target_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            class_mode='categorical')\n",
    "    \n",
    "   \n",
    "    validation_generator = data_generator_with_aug.flow_from_directory(\n",
    "            '../input/fire-detection-from-cctv/data/data/img_data/test',\n",
    "            target_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=TEST_BATCH_SIZE,\n",
    "            shuffle = False,\n",
    "            class_mode='categorical')\n",
    "    \n",
    "        \n",
    "    #y_train = get_labels(train_generator)\n",
    "    #weights = class_weight.compute_class_weight('balanced',np.unique(y_train), y_train)\n",
    "    #dict_weights = { i: weights[i] for i in range(len(weights)) }\n",
    "       \n",
    "    H = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.n/TRAIN_BATCH_SIZE,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=1 #,\n",
    "            #class_weight=dict_weights\n",
    "                )\n",
    "    \n",
    "    plot_history( H, NUM_EPOCHS )\n",
    "    \n",
    "    return model, train_generator,validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_dict(train_generator ):\n",
    "# Get label to class_id mapping\n",
    "    labels = (train_generator.class_indices)\n",
    "    label_dict = dict((v,k) for k,v in labels.items())\n",
    "    return  label_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels( generator ):\n",
    "    generator.reset()\n",
    "    labels = []\n",
    "    for i in range(len(generator)):\n",
    "        labels.extend(np.array(generator[i][1]) )\n",
    "    return np.argmax(labels, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_labels( test_generator):\n",
    "    test_generator.reset()\n",
    "    pred_vec=model.predict_generator(test_generator,\n",
    "                                     steps=test_generator.n, #test_generator.batch_size\n",
    "                                     verbose=1)\n",
    "    return np.argmax( pred_vec, axis = 1), np.max(pred_vec, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history( H, NUM_EPOCHS ):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(15, 5)\n",
    "    \n",
    "    fig.add_subplot(1, 3, 1)\n",
    "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.title(\"Training Loss and Validation Loss on Dataset\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    \n",
    "    fig.add_subplot(1, 3, 2)\n",
    "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    fig.add_subplot(1, 3, 3)\n",
    "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, NUM_EPOCHS), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Validation Loss and Accuracy on Dataset\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    #plt.savefig(\"plot.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Prediction involves sampling the test videos into frames and predicting the class probabilities in each sampled frame. The class with the maximum probbality is assigned to the frame. However, if the max probablility is less than 0.5, the frame is assigned to the default class.  The predicted class and the class probability is drawn on the frame before writing it into a separate video file using VideoWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction( frame, class_string ):\n",
    "    x_start = frame.shape[1] -600\n",
    "    cv2.putText(frame, class_string, (x_start, 75), cv2.FONT_HERSHEY_SIMPLEX, 2.5, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_for_prediction( img):\n",
    "   \n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    # The below function inserts an additional dimension at the axis position provided\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    # perform pre-processing that was done when resnet model was trained.\n",
    "    return preprocess_input(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_l, train_generator,validation_generator = train_model(model)\n",
    "label_dict_l = get_label_dict(train_generator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_s, train_generator,validation_generator = train_model(model)\n",
    "label_dict_s = get_label_dict(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_display_string(pred_class, label_dict):\n",
    "    txt = \"\"\n",
    "    for c, confidence in pred_class:\n",
    "        txt += label_dict[c]\n",
    "        if c :\n",
    "            txt += '['+ str(confidence) +']'\n",
    "    #print(\"count=\"+str(len(pred_class)) + \" txt:\" + txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(  model, video_path, filename, label_dict ):\n",
    "    \n",
    "    vs = cv2.VideoCapture(video_path)\n",
    "    fps = math.floor(vs.get(cv2.CAP_PROP_FPS))\n",
    "    ret_val = True\n",
    "    writer = 0\n",
    "    \n",
    "    while True:\n",
    "        ret_val, frame = vs.read()\n",
    "        if not ret_val:\n",
    "            break\n",
    "       \n",
    "        resized_frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "        frame_for_pred = prepare_image_for_prediction( resized_frame )\n",
    "        pred_vec = model.predict(frame_for_pred)\n",
    "        #print(pred_vec)\n",
    "        pred_class =[]\n",
    "        confidence = np.round(pred_vec.max(),2) \n",
    "        \n",
    "        if confidence > 0.4:\n",
    "            pc = pred_vec.argmax()\n",
    "            pred_class.append( (pc, confidence) )\n",
    "        else:\n",
    "            pred_class.append( (0, 0) )\n",
    "        if pred_class:\n",
    "            txt = get_display_string(pred_class, label_dict)       \n",
    "            frame = draw_prediction( frame, txt )\n",
    "        #print(pred_class)\n",
    "        #plt.axis('off')\n",
    "        #plt.imshow(frame)\n",
    "        #plt.show()\n",
    "        #clear_output(wait = True)\n",
    "        if not writer:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "            writer = cv2.VideoWriter(filename, fourcc, fps,(frame.shape[1], frame.shape[0]), True)\n",
    "            \n",
    "        # write the out\n",
    "        writer.write(frame)\n",
    "        \n",
    "    vs.release()\n",
    "    writer.release()\n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_lables = get_test_labels( validation_generator )\n",
    "#print(test_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_lables, confidence = get_pred_labels( validation_generator )\n",
    "#print( pred_lables )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '../input/fire-detection-from-cctv/data/data/video_data/test_videos/test1.mp4'\n",
    "predict ( trained_model_l, video_path, 'test1_9.avi',  label_dict_l) \n",
    "\n",
    "video_path = '../input/fire-detection-from-cctv/data/data/video_data/test_videos/test2.mp4'\n",
    "predict ( trained_model_l, video_path, 'test2_9.avi',  label_dict_l) \n",
    "\n",
    "video_path = '../input/fire-detection-from-cctv/data/data/video_data/test_videos/test3.mp4'\n",
    "predict ( trained_model_l, video_path, 'test3_9.avi',  label_dict_l) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "Below are the three test videos that were passed through the trained model. It can be seen that in case there is both fire and smoke, the model is biased towards predicting it as smoke. If we look at the training vs validtion accuracy, it is clear that the model is overfitting the training data. There is still some work that needs to be done on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('cHlTG6WL0OY', width=800, height=450)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('Lk7_qDy60CI', width=800, height=450)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1] https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/  \n",
    "[2] https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720  \n",
    "[3] https://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/  \n",
    "[4] https://github.com/bikz05/ipython-notebooks/blob/master/computer-vision/displaying-video-in-ipython-notebook.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
